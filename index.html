<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Microphone List</title>
    <style>
        body {
            margin: 0;
            padding: 16px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            font-size: 14px;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            /*border-radius: 8px;*/
            /*border: 1px solid rgba(0, 0, 0, 0.1);*/
            /*box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);*/
            max-height: 800px;
            overflow-y: auto;
        }

        .header {
            display: flex;
            align-items: center;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid rgba(0, 0, 0, 0.1);
        }

        .title {
            font-weight: 600;
            color: #333;
            margin: 0;
            flex: 1;
        }

        .refresh-btn {
            background: #007AFF;
            color: white;
            border: none;
            border-radius: 4px;
            padding: 4px 8px;
            font-size: 12px;
            cursor: pointer;
            transition: background 0.2s;
        }

        .refresh-btn:hover {
            background: #0056b3;
        }

        .mic-list {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .mic-item {
            display: flex;
            align-items: center;
            padding: 8px 12px;
            border-bottom: 1px solid rgba(0, 0, 0, 0.05);
            cursor: pointer;
            border-radius: 6px;
            margin: 2px 0;
            transition: all 0.2s ease;
            position: relative;
        }

        .mic-item:last-child {
            border-bottom: none;
        }

        .mic-item:hover {
            background: rgba(0, 122, 255, 0.1);
            transform: translateX(2px);
        }

        .mic-item.selected {
            background: rgba(0, 122, 255, 0.15);
            border: 1px solid rgba(0, 122, 255, 0.3);
        }

        .mic-item.selected::after {
            content: "‚úì";
            position: absolute;
            right: 12px;
            color: #007AFF;
            font-weight: bold;
            font-size: 16px;
        }

        .mic-icon {
            width: 16px;
            height: 16px;
            margin-right: 8px;
            opacity: 0.7;
        }

        .mic-info {
            flex: 1;
        }

        .mic-name {
            font-weight: 500;
            color: #333;
            margin: 0 0 2px 0;
            font-size: 13px;
        }

        .mic-id {
            font-size: 11px;
            color: #666;
            font-family: 'SF Mono', Monaco, monospace;
            margin: 0;
        }

        .status {
            padding: 8px 0;
            text-align: center;
            color: #666;
            font-size: 12px;
        }

        .error {
            color: #ff3333;
        }

        .loading {
            color: #007AFF;
        }

        .empty {
            color: #999;
            font-style: italic;
        }

        .virtual-mic-controls {
            margin-top: 16px;
            padding-top: 12px;
            border-top: 1px solid rgba(0, 0, 0, 0.1);
        }

        .virtual-output-select {
            margin-top: 8px;
        }

        .virtual-output-select select {
            width: 100%;
            padding: 6px 8px;
            border: 1px solid rgba(0, 0, 0, 0.2);
            border-radius: 4px;
            font-size: 12px;
            background: white;
        }

        .virtual-output-select label {
            display: block;
            font-size: 11px;
            color: #666;
            margin-bottom: 4px;
        }

        .setup-info {
            margin-top: 8px;
            padding: 8px;
            background: rgba(255, 193, 7, 0.1);
            border: 1px solid rgba(255, 193, 7, 0.3);
            border-radius: 4px;
            font-size: 11px;
            color: #856404;
        }

        .setup-info a {
            color: #007AFF;
            text-decoration: none;
        }

        .virtual-mic-status {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 8px;
        }

        .status-indicator {
            display: flex;
            align-items: center;
            font-size: 12px;
            font-weight: 500;
        }

        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            margin-right: 6px;
        }

        .status-dot.active {
            background: #00C853;
            box-shadow: 0 0 8px rgba(0, 200, 83, 0.4);
        }

        .status-dot.inactive {
            background: #666;
        }

        .virtual-mic-btn {
            background: #007AFF;
            color: white;
            border: none;
            border-radius: 6px;
            padding: 8px 16px;
            font-size: 12px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            width: 100%;
            margin-top: 8px;
        }

        .virtual-mic-btn:hover {
            background: #0056b3;
            transform: translateY(-1px);
        }

        .virtual-mic-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
        }

        .virtual-mic-btn.stop {
            background: #FF3B30;
        }

        .virtual-mic-btn.stop:hover {
            background: #d32f2f;
        }

        .transcription-section {
            margin-top: 16px;
            padding-top: 12px;
            border-top: 1px solid rgba(0, 0, 0, 0.1);
        }

        .transcription-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 12px;
        }

        .transcription-header h4 {
            margin: 0;
            font-size: 14px;
            font-weight: 600;
            color: #333;
        }

        .transcription-status {
            display: flex;
            align-items: center;
            font-size: 12px;
            font-weight: 500;
        }

        .api-key-input {
            margin-bottom: 12px;
        }

        .api-key-input label {
            display: block;
            font-size: 11px;
            color: #666;
            margin-bottom: 4px;
        }

        .api-key-input input {
            width: calc(100% - 60px);
            padding: 6px 8px;
            border: 1px solid rgba(0, 0, 0, 0.2);
            border-radius: 4px;
            font-size: 12px;
            margin-right: 8px;
        }

        .api-key-input button {
            width: 50px;
            padding: 6px 8px;
            background: #007AFF;
            color: white;
            border: none;
            border-radius: 4px;
            font-size: 11px;
            cursor: pointer;
        }

        .api-key-input button:hover {
            background: #0056b3;
        }

        .transcription-output {
            background: rgba(0, 0, 0, 0.02);
            border: 1px solid rgba(0, 0, 0, 0.1);
            border-radius: 6px;
            padding: 12px;
            min-height: 80px;
            max-height: 200px;
            overflow-y: auto;
        }

        .transcription-text {
            font-size: 12px;
            line-height: 1.4;
            color: #333;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        .transcription-text.empty {
            color: #999;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="header">
        <h3 class="title">üé§ Available Microphones</h3>
        <button class="refresh-btn" onclick="refreshMicrophones()">‚Üª</button>
    </div>
    
    <div id="status" class="status loading">Loading microphones...</div>
    <ul id="micList" class="mic-list"></ul>
    
    <div class="virtual-mic-controls">
        <div class="virtual-mic-status">
            <div class="status-indicator">
                <div id="virtualMicDot" class="status-dot inactive"></div>
                <span id="virtualMicStatusText">Virtual Mic: Stopped</span>
            </div>
        </div>
        
        <div class="virtual-output-select">
            <label for="virtualOutputSelect">Virtual Audio Output:</label>
            <select id="virtualOutputSelect">
                <option value="">Select virtual audio device...</option>
            </select>
        </div>
        
        <button id="virtualMicBtn" class="virtual-mic-btn" onclick="toggleVirtualMic()">
            Start Virtual Microphone
        </button>
        
        <div class="setup-info">
            ‚ÑπÔ∏è Need a virtual audio device? <a href="#" onclick="showSetupInfo()">Setup Guide</a>
        </div>
    </div>

    <div class="transcription-section">
        <div class="transcription-header">
            <h4>üéØ Live Transcription</h4>
            <div class="transcription-status">
                <div id="transcriptionDot" class="status-dot inactive"></div>
                <span id="transcriptionStatusText">Stopped</span>
            </div>
        </div>
        
        <div class="api-key-input">
            <label for="sonioxApiKey">Soniox API Key:</label>
            <input type="password" id="sonioxApiKey" placeholder="Enter your Soniox API key..." />
            <button id="saveApiKey" onclick="saveApiKey()">Save</button>
        </div>
        
        <div class="transcription-output">
            <div id="transcriptionText" class="transcription-text">
                Transcription will appear here when virtual microphone is active...
            </div>
        </div>
    </div>

    <script>
        // Override console to route all output to main process CLI IMMEDIATELY
        const originalConsole = {
            log: console.log,
            error: console.error,
            warn: console.warn
        };
        
        // Set up console override right away
        (function() {
            console.log = function(...args) {
                const message = args.join(' ');
                if (window.electronAPI) {
                    window.electronAPI.logToMain(message);
                } else {
                    originalConsole.log(...args);
                }
            };
            
            console.error = function(...args) {
                const message = args.join(' ');
                if (window.electronAPI) {
                    window.electronAPI.logError(message);
                } else {
                    originalConsole.error(...args);
                }
            };
            
            console.warn = function(...args) {
                const message = args.join(' ');
                if (window.electronAPI) {
                    window.electronAPI.logWarn(message);
                } else {
                    originalConsole.warn(...args);
                }
            };
        })();
        
        // Test console output immediately
        // Console output routing initialized

        let microphones = [];
        let selectedMicrophoneId = null;
        let virtualMicActive = false;
        let virtualMicrophone = null;
        let virtualOutputDevices = [];
        let selectedVirtualOutputId = null;

        // Function to detect virtual audio output devices
        async function getVirtualOutputDevices() {
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioOutputs = devices.filter(device => device.kind === 'audiooutput');
                
                // Look for common virtual audio device names
                const virtualDeviceNames = [
                    'blackhole', 'loopback', 'virtual', 'cable', 'voicemeeter', 'vb-audio',
                    'multi-output', 'aggregate', 'soundflower', 'jack'
                ];
                
                const virtualDevices = audioOutputs.filter(device => {
                    const name = device.label.toLowerCase();
                    return virtualDeviceNames.some(keyword => name.includes(keyword));
                });
                
                // Found virtual output devices
                return virtualDevices;
                
            } catch (error) {
                console.error('Error getting virtual output devices:', error);
                return [];
            }
        }

        async function populateVirtualOutputDevices() {
            const select = document.getElementById('virtualOutputSelect');
            const devices = await getVirtualOutputDevices();
            
            // Clear existing options except the first one
            select.innerHTML = '<option value="">Select virtual audio device...</option>';
            
            if (devices.length === 0) {
                const option = document.createElement('option');
                option.value = 'none';
                option.textContent = 'No virtual devices found - See setup guide';
                option.disabled = true;
                select.appendChild(option);
            } else {
                devices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.textContent = device.label || 'Unnamed Virtual Device';
                    
                    // Preselect the first virtual device
                    if (index === 0) {
                        option.selected = true;
                        selectedVirtualOutputId = device.deviceId;
                        // Preselected virtual device
                    }
                    
                    select.appendChild(option);
                });
            }
            
            virtualOutputDevices = devices;
        }

        function showSetupInfo() {
            const message = `To create a system-wide virtual microphone, you need to install a virtual audio driver:\n\n` +
                `macOS: Install BlackHole (free) from GitHub or Loopback (paid)\n` +
                `Windows: Install VoiceMeeter or Virtual Audio Cable\n` +
                `Linux: Use PulseAudio virtual sinks or JACK\n\n` +
                `See VIRTUAL_AUDIO_SETUP.md for detailed instructions.`;
            alert(message);
        }

        // Virtual Microphone Class  
        class VirtualMicrophone {
            constructor() {
                this.isActive = false;
                this.inputStream = null;
                this.audioContext = null;
                this.sourceNode = null;
                this.beepNode = null;
                this.beepGainNode = null;
                this.beepInterval = null;
                this.destinationNode = null;
                this.outputStream = null;
                this.outputAudio = null;
                this.transcriptionWs = null;
                this.transcriptionActive = false;
                this.finalTranscript = '';
                this.interimTranscript = '';
                this.mediaRecorder = null;
            }

            async initialize() {
                try {
                    // Don't create AudioContext immediately, create it when needed
                    return true;
                } catch (error) {
                    console.error('Failed to initialize virtual microphone:', error);
                    return false;
                }
            }

            async startTranscription() {
                let apiKey = null;
                
                // Try to get API key from environment first
                if (window.electronAPI) {
                    apiKey = await window.electronAPI.getSonioxApiKey();
                }
                
                // Fallback to localStorage if not in environment
                if (!apiKey) {
                    apiKey = localStorage.getItem('sonioxApiKey');
                }
                
                if (!apiKey) {
                    console.warn('Soniox API key not set. Skipping transcription.');
                    updateTranscriptionStatus('API Key Required', false);
                    return;
                }

                if (this.transcriptionActive) {
                    console.log('Transcription already active');
                    return;
                }

                try {
                    console.log('Starting Soniox transcription...');
                    updateTranscriptionStatus('Connecting...', false);

                    this.transcriptionWs = new WebSocket('wss://stt-rt.soniox.com/transcribe-websocket');
                    
                    this.transcriptionWs.onopen = () => {
                        console.log('Soniox WebSocket connected');
                        updateTranscriptionStatus('Active', true);
                        
                        // Send start request
                        this.transcriptionWs.send(JSON.stringify({
                            api_key: apiKey,
                            audio_format: "auto", // Let Soniox auto-detect the format
                            model: "stt-rt-preview",
                            language_hints: ["en"]
                        }));
                        
                        this.transcriptionActive = true;
                        this.finalTranscript = '';
                        this.interimTranscript = '';
                        
                        // Start capturing audio for transcription
                        this.startAudioCapture();
                    };

                    this.transcriptionWs.onmessage = (event) => {
                        try {
                            const response = JSON.parse(event.data);
                            this.handleTranscriptionResponse(response);
                        } catch (error) {
                            console.error('Error parsing transcription response:', error);
                        }
                    };

                    this.transcriptionWs.onerror = (error) => {
                        console.error('Transcription WebSocket error:', error);
                        updateTranscriptionStatus('Error', false);
                    };

                    this.transcriptionWs.onclose = () => {
                        console.log('Transcription WebSocket closed');
                        this.transcriptionActive = false;
                        updateTranscriptionStatus('Stopped', false);
                    };

                } catch (error) {
                    console.error('Error starting transcription:', error);
                    updateTranscriptionStatus('Error', false);
                }
            }

            handleTranscriptionResponse(response) {
                if (response.error_code) {
                    console.error(`Transcription error: ${response.error_code} ${response.error_message}`);
                    updateTranscriptionStatus('Error', false);
                    return;
                }

                let newInterimText = '';
                let hasNewFinalText = false;

                for (const token of response.tokens || []) {
                    if (token.text) {
                        if (token.is_final) {
                            this.finalTranscript += token.text;
                            hasNewFinalText = true;
                        } else {
                            newInterimText += token.text;
                        }
                    }
                }

                this.interimTranscript = newInterimText;
                this.updateTranscriptionDisplay();

                if (response.finished) {
                    console.log('Transcription finished');
                }
            }

            updateTranscriptionDisplay() {
                const transcriptionEl = document.getElementById('transcriptionText');
                if (transcriptionEl) {
                    const fullText = this.finalTranscript + this.interimTranscript;
                    transcriptionEl.textContent = fullText || 'Listening...';
                    transcriptionEl.classList.toggle('empty', !fullText);
                    
                    // Auto-scroll to bottom
                    transcriptionEl.scrollTop = transcriptionEl.scrollHeight;
                }
            }

            async startAudioCapture() {
                if (!this.audioContext || !this.sourceNode) {
                    console.warn('Audio context not available for transcription');
                    return;
                }

                try {
                    // Use a more stable approach with MediaRecorder for audio capture
                    if (this.inputStream) {
                        console.log('Setting up MediaRecorder for transcription...');
                        
                        // Create a MediaRecorder to capture audio
                        this.mediaRecorder = new MediaRecorder(this.inputStream, {
                            mimeType: 'audio/webm;codecs=opus',
                            audioBitsPerSecond: 16000
                        });
                        
                        this.mediaRecorder.ondataavailable = (event) => {
                            if (event.data.size > 0 && this.transcriptionActive && 
                                this.transcriptionWs && this.transcriptionWs.readyState === WebSocket.OPEN) {
                                
                                // Convert the blob to ArrayBuffer and send
                                const reader = new FileReader();
                                reader.onload = () => {
                                    if (this.transcriptionWs && this.transcriptionWs.readyState === WebSocket.OPEN) {
                                        this.transcriptionWs.send(reader.result);
                                    }
                                };
                                reader.readAsArrayBuffer(event.data);
                            }
                        };
                        
                        this.mediaRecorder.onerror = (error) => {
                            console.error('MediaRecorder error:', error);
                        };
                        
                        // Start recording with small intervals for real-time streaming
                        this.mediaRecorder.start(100); // 100ms chunks
                        console.log('Audio capture started for transcription');
                    }

                } catch (error) {
                    console.error('Error starting audio capture:', error);
                    // Fallback to a simple approach without real-time streaming
                    console.log('Falling back to basic transcription mode');
                }
            }

            stopTranscription() {
                if (this.transcriptionWs) {
                    if (this.transcriptionWs.readyState === WebSocket.OPEN) {
                        this.transcriptionWs.send(''); // Signal end of stream
                    }
                    this.transcriptionWs.close();
                    this.transcriptionWs = null;
                }

                if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
                    this.mediaRecorder.stop();
                    this.mediaRecorder = null;
                }

                if (this.audioProcessor) {
                    this.audioProcessor.disconnect();
                    this.audioProcessor = null;
                }

                this.transcriptionActive = false;
                updateTranscriptionStatus('Stopped', false);
                console.log('Transcription stopped');
            }

            async start(deviceId, outputDeviceId = null) {
                if (this.isActive) {
                    await this.stop();
                }

                try {
                    console.log('Starting virtual microphone with device:', deviceId);
                    console.log('Output device:', outputDeviceId);
                    
                    // Check if navigator.mediaDevices is available
                    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                        throw new Error('getUserMedia is not supported in this browser');
                    }
                    
                    // Get input stream from selected microphone
                    const constraints = {
                        audio: {
                            deviceId: deviceId ? { exact: deviceId } : undefined,
                            sampleRate: 44100,
                            channelCount: 1,
                            echoCancellation: false,
                            noiseSuppression: false,
                            autoGainControl: false
                        }
                    };

                    console.log('Requesting getUserMedia with constraints:', constraints);
                    this.inputStream = await navigator.mediaDevices.getUserMedia(constraints);
                    console.log('Got input stream:', this.inputStream);
                    
                    // Create AudioContext only when we have a stream
                    if (!this.audioContext) {
                        this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                            sampleRate: 44100,
                            latencyHint: 'interactive'
                        });
                        console.log('AudioContext created');
                    }
                    
                    // Resume audio context if suspended
                    if (this.audioContext.state === 'suspended') {
                        await this.audioContext.resume();
                        console.log('AudioContext resumed');
                    }
                    
                    // Create source node from input stream
                    this.sourceNode = this.audioContext.createMediaStreamSource(this.inputStream);
                    console.log('Created media stream source');
                    
                    // Create processing chain (simplified to avoid crashes)
                    const processedNode = this.createSimpleProcessingChain(this.sourceNode);
                    
                    // Create destination for virtual microphone
                    this.destinationNode = this.audioContext.createMediaStreamDestination();
                    processedNode.connect(this.destinationNode);
                    console.log('Connected processing chain');
                    
                    this.outputStream = this.destinationNode.stream;
                    
                    // If virtual output device is selected, route audio to it
                    if (outputDeviceId) {
                        await this.setupVirtualOutput(outputDeviceId);
                    }
                    
                    this.isActive = true;
                    
                    // Start transcription if API key is available
                    await this.startTranscription();
                    
                    console.log('Virtual microphone started with device:', deviceId);
                    console.log('Virtual microphone stream available:', this.outputStream);
                    
                    return true;
                    
                } catch (error) {
                    console.error('Failed to start virtual microphone:', error);
                    console.error('Error name:', error.name);
                    console.error('Error message:', error.message);
                    console.error('Error stack:', error.stack);
                    
                    // Clean up on error
                    if (this.inputStream) {
                        this.inputStream.getTracks().forEach(track => track.stop());
                        this.inputStream = null;
                    }
                    
                    return false;
                }
            }

            async setupVirtualOutput(outputDeviceId) {
                try {
                    // Create an audio element to play to the virtual output device
                    this.outputAudio = new Audio();
                    this.outputAudio.srcObject = this.outputStream;
                    this.outputAudio.muted = false;
                    this.outputAudio.volume = 1.0;
                    
                    // Set the output device if supported
                    if (this.outputAudio.setSinkId) {
                        await this.outputAudio.setSinkId(outputDeviceId);
                        console.log('Audio routed to virtual device:', outputDeviceId);
                    } else {
                        console.warn('setSinkId not supported - audio will play to default device');
                    }
                    
                    // Start playing to route audio to the virtual device
                    await this.outputAudio.play();
                    
                } catch (error) {
                    console.error('Error setting up virtual output:', error);
                    throw error;
                }
            }

            createSimpleProcessingChain(sourceNode) {
                try {
                    // Create beep generator only (no noise)
                    this.beepNode = this.createBeepNode();
                    
                    // Create gain nodes for mixing
                    const signalGain = this.audioContext.createGain();
                    const beepGain = this.audioContext.createGain();
                    const outputGain = this.audioContext.createGain();
                    
                    // Set gain values - 80% signal, 20% beep for louder beeps
                    signalGain.gain.value = 0.8;
                    beepGain.gain.value = 0.2;
                    outputGain.gain.value = 1.0;
                    
                    // Connect the audio graph
                    sourceNode.connect(signalGain);
                    this.beepNode.connect(beepGain);
                    
                    signalGain.connect(outputGain);
                    beepGain.connect(outputGain);
                    
                    console.log('Created processing chain with beeps only (no noise)');
                    
                    return outputGain;
                } catch (error) {
                    console.error('Error creating processing chain:', error);
                    // Fallback to simple gain node
                    const gainNode = this.audioContext.createGain();
                    gainNode.gain.value = 1.0;
                    sourceNode.connect(gainNode);
                    return gainNode;
                }
            }

            createNoiseNode() {
                try {
                    // Create a buffer for white noise
                    const bufferSize = this.audioContext.sampleRate * 2; // 2 seconds of noise
                    const noiseBuffer = this.audioContext.createBuffer(1, bufferSize, this.audioContext.sampleRate);
                    const output = noiseBuffer.getChannelData(0);
                    
                    // Generate white noise
                    for (let i = 0; i < bufferSize; i++) {
                        output[i] = Math.random() * 2 - 1; // Random values between -1 and 1
                    }
                    
                    // Create buffer source and set it to loop
                    const noiseSource = this.audioContext.createBufferSource();
                    noiseSource.buffer = noiseBuffer;
                    noiseSource.loop = true;
                    noiseSource.start(0);
                    
                    console.log('Created noise generator node');
                    
                    return noiseSource;
                } catch (error) {
                    console.error('Error creating noise node:', error);
                    // Return a silent gain node as fallback
                    const silentGain = this.audioContext.createGain();
                    silentGain.gain.value = 0;
                    return silentGain;
                }
            }

            createBeepNode() {
                try {
                    // Create a gain node to control beep volume
                    const beepGain = this.audioContext.createGain();
                    beepGain.gain.value = 1.0; // Set to 1.0 so beeps can be heard
                    
                    // Store reference for cleanup
                    this.beepGainNode = beepGain;
                    
                    // Start beep scheduling
                    this.startBeepScheduling();
                    
                    console.log('Created beep generator node with gain 1.0');
                    
                    return beepGain;
                } catch (error) {
                    console.error('Error creating beep node:', error);
                    // Return a silent gain node as fallback
                    const silentGain = this.audioContext.createGain();
                    silentGain.gain.value = 0;
                    return silentGain;
                }
            }

            startBeepScheduling() {
                try {
                    this.stopBeepScheduling(); // Clear any existing schedule
                    
                    const createBeep = () => {
                        if (!this.isActive || !this.beepGainNode) return;
                        
                        try {
                            // Create a new oscillator for each beep (oscillators are one-time use)
                            const oscillator = this.audioContext.createOscillator();
                            oscillator.frequency.value = 800; // 800Hz beep tone
                            oscillator.type = 'sine';
                            
                            // Create a dedicated gain node for this beep
                            const singleBeepGain = this.audioContext.createGain();
                            singleBeepGain.gain.value = 1.0; // Full volume for beep
                            
                            // Connect: oscillator -> beep gain -> main beep gain
                            oscillator.connect(singleBeepGain);
                            singleBeepGain.connect(this.beepGainNode);
                            
                            const currentTime = this.audioContext.currentTime;
                            const beepDuration = 0.2; // 200ms beep for better audibility
                            
                            // Start and stop the oscillator
                            oscillator.start(currentTime);
                            oscillator.stop(currentTime + beepDuration);
                            
                            // Clean up after beep ends
                            oscillator.onended = () => {
                                try {
                                    singleBeepGain.disconnect();
                                } catch (e) {
                                    // Ignore cleanup errors
                                }
                            };
                            
                            console.log('Beep triggered at', currentTime);
                            
                        } catch (error) {
                            console.error('Error creating individual beep:', error);
                        }
                    };
                    
                    // Create beeps every 3 seconds
                    this.beepInterval = setInterval(createBeep, 3000);
                    
                    // Create the first beep immediately
                    setTimeout(createBeep, 100); // Small delay to ensure audio context is ready
                    
                    console.log('Started beep scheduling every 3 seconds');
                    
                } catch (error) {
                    console.error('Error starting beep scheduling:', error);
                }
            }

            stopBeepScheduling() {
                if (this.beepInterval) {
                    clearInterval(this.beepInterval);
                    this.beepInterval = null;
                    console.log('Stopped beep scheduling');
                }
            }


            async stop() {
                if (!this.isActive) {
                    return;
                }

                try {
                    // Set isActive to false first to stop beep scheduling
                    this.isActive = false;
                    
                    // Stop transcription
                    this.stopTranscription();
                    
                    // Stop beep scheduling
                    this.stopBeepScheduling();
                    
                    if (this.outputAudio) {
                        this.outputAudio.pause();
                        this.outputAudio.srcObject = null;
                        this.outputAudio = null;
                    }
                    
                    if (this.inputStream) {
                        this.inputStream.getTracks().forEach(track => track.stop());
                        this.inputStream = null;
                    }
                    
                    if (this.sourceNode) {
                        this.sourceNode.disconnect();
                        this.sourceNode = null;
                    }
                    
                    if (this.beepNode) {
                        this.beepNode.disconnect();
                        this.beepNode = null;
                    }
                    
                    if (this.beepGainNode) {
                        this.beepGainNode.disconnect();
                        this.beepGainNode = null;
                    }
                    
                    if (this.destinationNode) {
                        this.destinationNode = null;
                    }
                    
                    this.outputStream = null;
                    
                    console.log('Virtual microphone stopped');
                    
                } catch (error) {
                    console.error('Error stopping virtual microphone:', error);
                }
            }

            isRunning() {
                return this.isActive;
            }
        }

        // Helper functions for transcription UI
        function updateTranscriptionStatus(status, isActive) {
            const statusText = document.getElementById('transcriptionStatusText');
            const statusDot = document.getElementById('transcriptionDot');
            
            if (statusText) statusText.textContent = status;
            if (statusDot) {
                statusDot.classList.toggle('active', isActive);
                statusDot.classList.toggle('inactive', !isActive);
            }
        }

        function saveApiKey() {
            const apiKeyInput = document.getElementById('sonioxApiKey');
            const apiKey = apiKeyInput.value.trim();
            
            if (apiKey) {
                localStorage.setItem('sonioxApiKey', apiKey);
                console.log('Soniox API key saved');
                
                // Clear the input for security
                apiKeyInput.value = '';
                alert('API key saved successfully!');
            } else {
                alert('Please enter a valid API key');
            }
        }

        async function loadSavedApiKey() {
            let hasApiKey = false;
            
            // Check environment variable first
            if (window.electronAPI) {
                const envApiKey = await window.electronAPI.getSonioxApiKey();
                if (envApiKey) {
                    hasApiKey = true;
                    console.log('Using Soniox API key from environment');
                }
            }
            
            // Check localStorage as fallback
            if (!hasApiKey) {
                const savedApiKey = localStorage.getItem('sonioxApiKey');
                if (savedApiKey) {
                    hasApiKey = true;
                    console.log('Using Soniox API key from localStorage');
                }
            }
            
            if (hasApiKey) {
                // Don't populate the input field for security, just indicate it's saved
                const statusText = document.getElementById('transcriptionStatusText');
                if (statusText && statusText.textContent === 'Stopped') {
                    statusText.textContent = 'Ready';
                }
                
                // Hide the API key input section since we have it from environment
                if (window.electronAPI) {
                    const apiKeySection = document.querySelector('.api-key-input');
                    if (apiKeySection) {
                        apiKeySection.style.display = 'none';
                    }
                }
            }
        }

        // Virtual microphone control functions
        async function toggleVirtualMic() {
            const btn = document.getElementById('virtualMicBtn');
            const statusText = document.getElementById('virtualMicStatusText');
            const statusDot = document.getElementById('virtualMicDot');
            
            btn.disabled = true;
            
            try {
                console.log('toggleVirtualMic called, current state:', virtualMicActive);
                if (!virtualMicActive) {
                    if (!selectedMicrophoneId) {
                        alert('Please select a microphone first');
                        btn.disabled = false;
                        return;
                    }
                    
                    statusText.textContent = 'Virtual Mic: Starting...';
                    
                    // Get selected virtual output device
                    const outputSelect = document.getElementById('virtualOutputSelect');
                    selectedVirtualOutputId = outputSelect.value || null;
                    
                    const success = await virtualMicrophone.start(selectedMicrophoneId, selectedVirtualOutputId);
                    
                    if (success) {
                        virtualMicActive = true;
                        btn.textContent = 'Stop Virtual Microphone';
                        btn.classList.add('stop');
                        statusText.textContent = 'Virtual Mic: Active';
                        statusDot.classList.remove('inactive');
                        statusDot.classList.add('active');
                    } else {
                        statusText.textContent = 'Virtual Mic: Failed to start';
                        setTimeout(() => {
                            statusText.textContent = 'Virtual Mic: Stopped';
                        }, 2000);
                    }
                } else {
                    statusText.textContent = 'Virtual Mic: Stopping...';
                    await virtualMicrophone.stop();
                    
                    virtualMicActive = false;
                    btn.textContent = 'Start Virtual Microphone';
                    btn.classList.remove('stop');
                    statusText.textContent = 'Virtual Mic: Stopped';
                    statusDot.classList.remove('active');
                    statusDot.classList.add('inactive');
                }
            } catch (error) {
                console.error('Virtual microphone error:', error);
                console.error('Error details:', {
                    name: error.name,
                    message: error.message,
                    stack: error.stack
                });
                
                statusText.textContent = `Virtual Mic: Error - ${error.message}`;
                setTimeout(() => {
                    statusText.textContent = 'Virtual Mic: Stopped';
                }, 3000);
                
                // Reset state on error
                virtualMicActive = false;
                btn.textContent = 'Start Virtual Microphone';
                btn.classList.remove('stop');
                statusDot.classList.remove('active');
                statusDot.classList.add('inactive');
            }
            
            btn.disabled = false;
        }

        async function updateVirtualMicStatus() {
            try {
                const isActive = virtualMicrophone ? virtualMicrophone.isRunning() : false;
                const statusText = document.getElementById('virtualMicStatusText');
                const statusDot = document.getElementById('virtualMicDot');
                const btn = document.getElementById('virtualMicBtn');
                
                if (isActive !== virtualMicActive) {
                    virtualMicActive = isActive;
                    
                    if (isActive) {
                        btn.textContent = 'Stop Virtual Microphone';
                        btn.classList.add('stop');
                        statusText.textContent = 'Virtual Mic: Active';
                        statusDot.classList.remove('inactive');
                        statusDot.classList.add('active');
                    } else {
                        btn.textContent = 'Start Virtual Microphone';
                        btn.classList.remove('stop');
                        statusText.textContent = 'Virtual Mic: Stopped';
                        statusDot.classList.remove('active');
                        statusDot.classList.add('inactive');
                    }
                }
            } catch (error) {
                console.error('Error checking virtual mic status:', error);
            }
        }

        function selectMicrophone(deviceId, element) {
            // Remove selected class from all items
            document.querySelectorAll('.mic-item').forEach(item => {
                item.classList.remove('selected');
            });
            
            // Add selected class to clicked item
            element.classList.add('selected');
            
            // Store selected microphone
            selectedMicrophoneId = deviceId;
            localStorage.setItem('selectedMicrophone', deviceId);
            
            console.log('Selected microphone:', deviceId);
        }

        async function getMicrophones() {
            try {
                const statusEl = document.getElementById('status');
                const listEl = document.getElementById('micList');
                
                statusEl.textContent = 'Loading microphones...';
                statusEl.className = 'status loading';
                listEl.innerHTML = '';

                // Request microphone permissions first
                await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Get all audio input devices
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioInputs = devices.filter(device => device.kind === 'audioinput');
                
                microphones = audioInputs;
                
                if (audioInputs.length === 0) {
                    statusEl.textContent = 'No microphones found';
                    statusEl.className = 'status empty';
                    return;
                }

                statusEl.style.display = 'none';
                
                // Load previously selected microphone
                const savedSelection = localStorage.getItem('selectedMicrophone');
                let hasSelection = false;
                let defaultMicrophone = null;
                
                // Find the default microphone (device with deviceId === 'default' or empty deviceId)
                const defaultDevice = audioInputs.find(device => 
                    device.deviceId === 'default' || device.deviceId === '' || 
                    device.label.toLowerCase().includes('default')
                );
                
                if (defaultDevice) {
                    defaultMicrophone = defaultDevice;
                } else {
                    // If no explicit default found, use the first microphone
                    defaultMicrophone = audioInputs[0];
                }
                
                // Display microphones
                audioInputs.forEach((device, index) => {
                    const li = document.createElement('li');
                    li.className = 'mic-item';
                    
                    const deviceName = device.label || `Microphone ${index + 1}`;
                    const deviceId = device.deviceId;
                    
                    // Check if this was the previously selected microphone
                    if (savedSelection === deviceId) {
                        li.classList.add('selected');
                        selectedMicrophoneId = deviceId;
                        hasSelection = true;
                    }
                    // Select default microphone if no previous selection
                    else if (!savedSelection && device === defaultMicrophone) {
                        li.classList.add('selected');
                        selectedMicrophoneId = deviceId;
                        localStorage.setItem('selectedMicrophone', deviceId);
                        hasSelection = true;
                    }
                    
                    li.innerHTML = `
                        <svg class="mic-icon" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                            <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                        </svg>
                        <div class="mic-info">
                            <p class="mic-name">${deviceName}</p>
                            <p class="mic-id">${deviceId.substring(0, 20)}...</p>
                        </div>
                    `;
                    
                    // Add click handler
                    li.addEventListener('click', () => {
                        selectMicrophone(deviceId, li);
                    });
                    
                    listEl.appendChild(li);
                });

            } catch (error) {
                console.error('Error getting microphones:', error);
                const statusEl = document.getElementById('status');
                statusEl.textContent = `Error: ${error.message}`;
                statusEl.className = 'status error';
            }
        }

        function refreshMicrophones() {
            getMicrophones();
        }

        // Global error handlers
        window.addEventListener('error', (event) => {
            console.error('Global error:', event.error);
            console.error('Error details:', {
                message: event.message,
                filename: event.filename,
                lineno: event.lineno,
                colno: event.colno,
                error: event.error
            });
        });

        window.addEventListener('unhandledrejection', (event) => {
            console.error('Unhandled promise rejection:', event.reason);
            event.preventDefault(); // Prevent the app from crashing
        });

        // Load microphones when the page loads
        document.addEventListener('DOMContentLoaded', async () => {
            try {
                console.log('DOM loaded, initializing...');
                
                // Initialize virtual microphone
                virtualMicrophone = new VirtualMicrophone();
                const initSuccess = await virtualMicrophone.initialize();
                console.log('Virtual microphone initialized:', initSuccess);
                
                getMicrophones();
                await populateVirtualOutputDevices();
                updateVirtualMicStatus();
                
                // Initialize transcription UI
                await loadSavedApiKey();
                updateTranscriptionStatus('Stopped', false);
                
                // Check virtual mic status periodically
                setInterval(updateVirtualMicStatus, 1000);
                
                console.log('Initialization complete');
            } catch (error) {
                console.error('Error during initialization:', error);
            }
        });

        // Refresh microphones when devices change
        navigator.mediaDevices.addEventListener('devicechange', () => {
            getMicrophones();
        });
    </script>
</body>
</html>